{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Jupyter notebook loads the raw amplitude and Mel spectrogram data files as numpy arrays.\n",
    "\n",
    "Download the data files [here](https://console.cloud.google.com/storage/browser/cs181_practical_data).  This notebook assumes that the data files as located in the same directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import balanced_accuracy_score # takes into account class sizes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load raw amplitude data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load train data\n",
    "\n",
    "X_amp_train = np.load(\"Xtrain_amp.npy\")\n",
    "y_amp_train = np.load(\"ytrain_amp.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_amp_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load test data\n",
    "\n",
    "X_amp_test = np.load(\"Xtest_amp.npy\")\n",
    "y_amp_test = np.load(\"ytest_amp.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_amp_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Mel spectrogram data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load train data\n",
    "\n",
    "X_mel_train = np.load(\"Xtrain_mel.npy\")\n",
    "y_mel_train = np.load(\"ytrain_mel.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_mel_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten X_mel_train's spectrogram features\n",
    "X_mel_train_flat = X_mel_train.reshape(X_mel_train.shape[0], -1)\n",
    "X_mel_train_flat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load test data\n",
    "\n",
    "X_mel_test = np.load(\"Xtest_mel.npy\")\n",
    "y_mel_test = np.load(\"ytest_mel.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_mel_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Perform PCA on raw amplitude features\n",
    "Compute a dimensionality reduction on the Xtrain_amp dataset and on the Xtest_amp dataset. This will allow us to not overload our model when training a logistic regression model with noisy data and only keep the 500 most significant components, which capture most of the variation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make some exploratory plots of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a plot of some of the raw frequencies so that we can explore the data\n",
    "plt.figure(dpi=150, figsize=(12,4))\n",
    "for i in range(3):\n",
    "    \n",
    "    # create our subplot\n",
    "    plt.subplot(1,3,i+1)\n",
    "    \n",
    "    # plot our image\n",
    "    plt.imshow(Xtrain_amp[i])\n",
    "    \n",
    "    # put our word label based on the 0 or 1\n",
    "    if ytrain_amp[i] == 0:\n",
    "        plt.xlabel(\"air_conditioner\")\n",
    "    elif ytrain_amp[i] == 1:\n",
    "        plt.xlabel(\"car_horn\")\n",
    "    elif ytrain_amp[i] == 2:\n",
    "        plt.xlabel(\"children_playing\")\n",
    "    elif ytrain_amp[i] == 3:\n",
    "        plt.xlabel(\"dog_bark\")\n",
    "    elif ytrain_amp[i] == 4:\n",
    "        plt.xlabel(\"drilling\")\n",
    "    elif ytrain_amp[i] == 5:\n",
    "        plt.xlabel(\"engine_idling\")\n",
    "    elif ytrain_amp[i] == 6:\n",
    "        plt.xlabel(\"gun_shot\")\n",
    "    elif ytrain_amp[i] == 7:\n",
    "        plt.xlabel(\"jackhammer\")\n",
    "    elif ytrain_amp[i] == 8:\n",
    "        plt.xlabel(\"siren\")\n",
    "    else:\n",
    "        plt.xlabel(\"street_music\")\n",
    "        \n",
    "    # beautify\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    \n",
    "# more beautifying\n",
    "plt.suptitle(\"Select Raw Aplitude Images\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train a logistic regression model on the 500 most significant PCA components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create our PCA object that will calculate the first two components.\n",
    "pca = PCA(n_components=500)\n",
    "\n",
    "# fit our PCA to the training data, and transform our data into its 500-dimensional representation\n",
    "# only want to fit to training data, we should not be getting any extra information from our test set\n",
    "Xtrain_amp_pca = pca.fit_transform(Xtrain_amp)\n",
    "\n",
    "# let's check the shape of our lower dim representation\n",
    "print(f\"Lower-Dim PCA Representation Train Data Shape: {Xtrain_amp_pca.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualize the first two components of the PCA to see what is being captured in the lower dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scatter plot the two components just to see if the data is separable by eye\n",
    "plt.figure(dpi=150, figsize=(8,6))\n",
    "\n",
    "# iterate through all the digits\n",
    "for sound_class in range(10):\n",
    "    \n",
    "    # get the samples that correspond to this digit\n",
    "    samples = Xtrain_amp_pca[ytrain_amp == sound_class]\n",
    "    \n",
    "    # scatter plot the first two PCA component\n",
    "    plt.scatter(samples[:,0], samples[:,1], label=str(sound_class))\n",
    "\n",
    "# beautify\n",
    "plt.xlabel(\"Principal Component 1\")\n",
    "plt.ylabel(\"Principal Component 2\")\n",
    "plt.legend(loc=\"upper right\", ncol=2)\n",
    "plt.title(\"2-Component PCA Visualization of Sound Classes\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create our LogisticRegression classifier\n",
    "lr_pca = LogisticRegression()\n",
    "\n",
    "# fit our SVM model\n",
    "model.fit(Xtrain_amp_pca, ytrain_amp)\n",
    "\n",
    "# scale our testing X into a lower dimension\n",
    "Xtest_amp_pca = pca.fit_transform(Xtest_amp)\n",
    "\n",
    "# make our predictions          \n",
    "lr_preds = model.predict(Xtest_amp_pca)\n",
    "          \n",
    "# calculate + display our accuracy\n",
    "lr_accuracy = np.mean(ytest_amp == lr_preds)\n",
    "print(\"Accuracy: \", lr_accuracy)\n",
    "\n",
    "\n",
    "# calculate and report: precision, recall, f1, support\n",
    "target_names = ['air_conditioner', 'car_horn', 'children_playing', 'dog_bark', 'drilling', 'engine_idling', 'gun_shot', 'jackhammer', 'siren', 'street_music']\n",
    "print(classification_report(ytest_amp, lr_preds, target_names=target_names))\n",
    "\n",
    "# calculate balanced accuracy, taking into account class sizes\n",
    "print(\"Balanced Accuracy: \", round(balanced_accuracy_score(ytest_amp, lr_preds), 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform PCA on the Mel spectogram features\n",
    "Train a logistic regression model on the 500 most significant PCA components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# flatten the dataset, since SVM cannot take into account matrices\n",
    "Xtrain_mel_flattened = Xtrain_mel.reshape(Xtrain_mel.shape[0], -1)\n",
    "Xtest_mel_flattened = Xtest_mel.reshape(Xtest_mel.shape[0], -1)\n",
    "\n",
    "# transform the dataset using PCA\n",
    "Xtrain_mel_pca = pca.fit_transform(Xtrain_mel_flattened)\n",
    "\n",
    "# fit the logistic regression model on the lower dimensional training data\n",
    "lr_pca.fit(Xtrain_mel_pca, ytrain_mel)\n",
    "\n",
    "# transform our test dataset into a lower dimension\n",
    "Xtest_mel_pca = pca.fit_transform(Xtest_mel_flattened)\n",
    "\n",
    "# make predictions for our test dataset from our Logistic regression fit on our train data\n",
    "preds_mel = lr_pca.predict(Xtest_mel_pca)\n",
    "\n",
    "# print the accuracy\n",
    "print(f\"Accuracy: {np.mean(preds_mel == ytest_mel)}\")\n",
    "\n",
    "# calculate and report: precision, recall, f1, support\n",
    "target_names = ['air_conditioner', 'car_horn', 'children_playing', 'dog_bark', 'drilling', 'engine_idling', 'gun_shot', 'jackhammer', 'siren', 'street_music']\n",
    "print(classification_report(ytest_mel, preds_mel, target_names=target_names))\n",
    "\n",
    "# calculate balanced accuracy, taking into account class sizes\n",
    "print(\"Balanced Accuracy: \", round(balanced_accuracy_score(ytest_mel, preds_mel), 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train nonlinear models on spectogram data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First Model: Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a model\n",
    "# select the hyperparameters we think appropriate\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(in_features=44100, out_features=441),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(in_features=441, out_features=10),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tell PyTorch to not track gradients\n",
    "with torch.no_grad():\n",
    "    \n",
    "    # convert our input data to a float tensor\n",
    "    # cannot pass numpy array into pytorch, have to BOTH introduce with torch.tensor but ALSO\n",
    "    # have to convert to float\n",
    "    inputs = torch.tensor(Xtest_amp).float()\n",
    "    \n",
    "    # pass our inputs straight into the model\n",
    "    preds = model(inputs)\n",
    "    \n",
    "    # determine what shape the predictions are so we know what to do with them\n",
    "    print(f\"Shape of preds: {preds.shape}\")\n",
    "\n",
    "\n",
    "    # no need to softmax the output since we will just take the argmax\n",
    "    # argmax and softmax would have the same highest value\n",
    "    preds = preds.argmax(1)\n",
    "    \n",
    "    # check test accuracy\n",
    "    print(f\"test accuracy: {torch.sum(preds == torch.tensor(ytest_amp)) / ytest_amp.shape[0]}\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# moving the model to right device\n",
    "model.to(device)\n",
    "\n",
    "# specify loss function\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "\n",
    "# define our optimizer \n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# metrics we want to collect\n",
    "train_accuracy_list = []\n",
    "train_loss_list = []\n",
    "test_accuracy_list = []\n",
    "test_loss_list = []\n",
    "\n",
    "\n",
    "# train for a given number of epochs\n",
    "for epoch in tqdm(range(20), desc=\"Epoch\"):\n",
    "        \n",
    "    # get our input imgs and labels. We have to convert from NumPy (original) to torch.tensors()\n",
    "    # PyTorch expects inputs as floats, and labels as longs (i.e., high-memory integers)\n",
    "    inputs = torch.tensor(Xtrain_amp).float()\n",
    "    labels = torch.tensor(ytrain_amp).long()\n",
    "    \n",
    "    # move our inputs and labels to the right device.\n",
    "    inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "    # reset the gradient \n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # compute our forward propogation\n",
    "    # outputs are not softmaxed\n",
    "    outputs = model(inputs) \n",
    "    \n",
    "    # calculate the cross-entropy loss\n",
    "    # this automatically applies softmax\n",
    "    loss = loss_func(outputs, labels) \n",
    "    \n",
    "    # calculate backpropogation\n",
    "    # takes the gradient of the loss function w.r.t. all the parameters\n",
    "    loss.backward() \n",
    "    \n",
    "    # make a small step update to our parameters\n",
    "    optimizer.step() \n",
    "    \n",
    "\n",
    "    # update our train_loss\n",
    "    # only inclue the .item() to extract the pure value from the tensor\n",
    "     train_loss_list.append(loss.item()) \n",
    "\n",
    "    # calculate + record our train + test accuracy. \n",
    "    # we have finished training here, only taking eval metrics\n",
    "    with torch.no_grad():\n",
    "\n",
    "        # get our predictions with the current weights\n",
    "        # we only want the indices, not the values\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        \n",
    "        # get our train accuracy\n",
    "        train_accuracy = torch.sum(predicted == labels) / labels.size(0)\n",
    "        \n",
    "        # add to our list of accuracies\n",
    "        train_accuracy_list.append(train_accuracy)\n",
    "            \n",
    "        # get our test inputs and labels same process as earlier\n",
    "        test_inputs = torch.tensor(Xtest_amp).float()\n",
    "        test_labels = torch.tensor(ytest_amp).long()\n",
    "\n",
    "        # move to the right device\n",
    "        test_inputs, test_labels = test_inputs.to(device), test_labels.to(device)\n",
    "\n",
    "        # run our test set inputs through the network\n",
    "        test_outputs = model(test_inputs)\n",
    "\n",
    "        # get our test_loss\n",
    "        test_loss = loss_func(test_outputs, test_labels)\n",
    "\n",
    "        # record our test_loss\n",
    "        # .item() tells pytorch we only want the value\n",
    "        test_loss_list.append(test_loss.item())\n",
    "\n",
    "        # make our predictions based on max\n",
    "        _, test_predicted = torch.max(test_outputs.data, 1)\n",
    "\n",
    "        # get our train accuracy\n",
    "        test_accuracy = torch.sum(test_predicted == test_labels) / test_labels.size(0)\n",
    "        \n",
    "        # add to our list\n",
    "        test_accuracy_list.append(test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(16, 5), dpi=200)\n",
    "\n",
    "ax[0].plot(train_accuracy_list, label=\"Train Accuracy\")\n",
    "ax[0].plot(test_accuracy_list, label=\"Test Accuracy\")\n",
    "ax[0].set_title(\"Accuracy Over Epochs\")\n",
    "ax[0].legend()\n",
    "\n",
    "ax[1].plot(train_loss_list, label=\"Train Loss\")\n",
    "ax[1].plot(test_loss_list, label=\"Test Loss\")\n",
    "ax[1].set_title(\"Loss Over Epochs\")\n",
    "ax[1].legend()\n",
    "\n",
    "plt.suptitle(\"Simple nn.Sequential Fully-Connected Neural Network\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Second Model: Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "# Try out a random forest model for classification\n",
    "# RandomForest\n",
    "model = RandomForestClassifier(n_estimators=500, max_depth=None, n_jobs=-1)\n",
    "model.fit(Xtrain_amp, ytrain_amp)\n",
    "\n",
    "# make our predictions\n",
    "rf_preds = model.predict(Xtest_amp)\n",
    "print(np.mean(rf_preds == ytest_amp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Third Model: SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "# create our SVM classifier\n",
    "# C is proxy of soft or hard margin\n",
    "model = SVC(C=2.0, kernel=\"rbf\")\n",
    "\n",
    "# SVM requires that each datapoint be a vector, and not a picture / matrix of pixels\n",
    "# use Xtrain_mel_flattened from above\n",
    "# fit our SVM model\n",
    "model.fit(Xtrain_mel_flattened, ytrain_mel)\n",
    "\n",
    "# make our predictions\n",
    "# must also use the Xtest_mel_flattened\n",
    "SVM_preds = model.predict(Xtest_mel_flattened)\n",
    "          \n",
    "# calculate + print our accuracy\n",
    "SVM_accuracy = np.mean(ytest_bc == SVM_preds)\n",
    "print(SVM_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter Tuning and Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform a hyperparameter search to maximize predictive accuracy for two model classes of your choice. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GridSearch for SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# these are the settings that we will tune: 'C', 'kernel'\n",
    "param_grid = {'C' : [0.01, 0.05, 0.1, 0.5, 1.0], \n",
    "              'kernel' : ['rbf', 'sigmoid'],}\n",
    "\n",
    "SVM = SVC()\n",
    "\n",
    "# instantiate our gridsearch estimator\n",
    "# cv=None defaults to the 5-fold cross validation\n",
    "SVM_CV = GridSearchCV(estimator=SVM, param_grid=param_grid, n_jobs=-1, cv=None, verbose=1)\n",
    "SVM_CV.fit(Xtrain_mel_flattened, ytrain_mel)\n",
    "\n",
    "# for each of possible combinations, trains on 9/10, test on 1/10\n",
    "\n",
    "# convert our results to a pd.DataFrame\n",
    "SVM_results = pd.DataFrame(SVM_CV.cv_results_).sort_values(by=['rank_test_score'])\n",
    "SVM_results.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GridSearch for RandomForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {'n_estimators' : [50, 100, 500, 1000], \n",
    "              'max_depth' : ['None', ],}\n",
    "\n",
    "# instantiate the model\n",
    "RF= RandomForestClassifier()\n",
    "\n",
    "# instantiate our gridsearch estimator\n",
    "RF_CV = GridSearchCV(estimator=RF, param_grid=param_grid, n_jobs=-1, cv=None, verbose=1)\n",
    "RF_CV.fit(Xtrain_mel_flattened, ytrain_mel)\n",
    "\n",
    "# convert results to pd.DataFrame\n",
    "RF_results = pd.DataFrame(RF_CV.cv_results_).sort_values(by=['rank_test_score'])\n",
    "RF_results.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN on Spectogram Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inherits the nn.Module class\n",
    "class CNN(nn.Module):\n",
    "    # only need the constructor for what layers you want \n",
    "    # and forward for how you want to feed forward\n",
    "    # everything else for backprop, etc is inherited from nn.Module\n",
    "    \n",
    "    # constructor\n",
    "    def __init__(self):\n",
    "        \n",
    "        # call the parent constructor (for nn.Module)\n",
    "        super().__init__()\n",
    "        \n",
    "        ## first set of convolution + pooling: see documentation about specifics\n",
    "        # conv2d = built in CNN\n",
    "        # out_channels = we will find 16 filters\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=16, kernel_size=3)\n",
    "        \n",
    "        ## to increase robustness ... again, this is optional reading\n",
    "        self.pool1 = nn.MaxPool2d(2)\n",
    "        \n",
    "        # second set of convolution + pooling\n",
    "        self.conv2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3)\n",
    "        self.pool2 = nn.MaxPool2d(2)\n",
    "        \n",
    "        # dropout layer to randomly drop neurons: for regularization\n",
    "        # randomly drops out parameters so that we don't overfit\n",
    "        self.dropout = nn.Dropout()\n",
    "        \n",
    "        ## end with three linear layers, this is general practice\n",
    "        self.fc1 = nn.Linear(800, 500)\n",
    "        self.fc2 = nn.Linear(500, 100)\n",
    "        self.fc3 = nn.Linear(100, 10)\n",
    "    \n",
    "    # define how the neural network processes ONE data point (which generalizes instantly into a whole dataset)\n",
    "    # aka: given x, how do we get the output? Just pass it through the layers.\n",
    "    def forward(self, x):\n",
    "        \n",
    "        # first set of convolution + pooling\n",
    "        x = self.conv1(x)\n",
    "        x = self.pool1(x)\n",
    "        x = torch.relu(x)\n",
    "        \n",
    "        # second set of convolution + pooling\n",
    "        x = self.conv2(x)\n",
    "        x = self.pool2(x)\n",
    "        x = torch.relu(x)\n",
    "        \n",
    "        # the drop out layer\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # flatten + linear layers\n",
    "        x = torch.flatten(x, start_dim=1)\n",
    "        x = self.fc1(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.fc3(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate our model\n",
    "model_cnn = CNN()\n",
    "\n",
    "# moving the model to right device\n",
    "model_cnn.to(device)\n",
    "\n",
    "# specify our loss function\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "\n",
    "# define our optimizer - could also do Adam, RMSprop, SGD\n",
    "optimizer = optim.Adam(model_cnn.parameters(), lr=1e-3, weight_decay=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# metrics we want to collect\n",
    "train_accuracy_list = []\n",
    "train_loss_list = []\n",
    "test_accuracy_list = []\n",
    "test_loss_list = []\n",
    "\n",
    "\n",
    "# train for a given number of epochs\n",
    "for epoch in tqdm(range(20), desc=\"Epoch\"):\n",
    "        \n",
    "    # query data for inputs (images) + labels: [inputs, labels]\n",
    "    inputs = torch.tensor(Xtrain_mel).float()\n",
    "    labels = torch.tensor(ytrain_mel).long()\n",
    "    \n",
    "    # move to the right device.\n",
    "    inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "    # reset the gradient\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # forward prop, backward prop, make incremental step\n",
    "    outputs = model_cnn(inputs) # implicitly calls the .forward() function\n",
    "    loss = loss_func(outputs, labels) # calculate the loss\n",
    "    loss.backward() # calculate the gradient\n",
    "    optimizer.step() # take our incremental update of the parameters\n",
    "\n",
    "    # update our train_loss\n",
    "    train_loss_list.append(loss.item()) \n",
    "\n",
    "    # calculate + record our train + test accuracy (TRAINING!)\n",
    "    with torch.no_grad():\n",
    "\n",
    "        # get our predictions with the current weights: torch.max returns values, indices\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        \n",
    "        # get our train accuracy\n",
    "        train_accuracy = torch.sum(predicted == labels) / labels.size(0)\n",
    "        \n",
    "        # add to our list\n",
    "        train_accuracy_list.append(train_accuracy)\n",
    "            \n",
    "        # get our test inputs and labels\n",
    "        test_inputs = torch.tensor(Xtest_digits_nn).float()\n",
    "        test_labels = torch.tensor(ytest_digits).long()\n",
    "\n",
    "        # move to the right device\n",
    "        test_inputs, test_labels = test_inputs.to(device), test_labels.to(device)\n",
    "\n",
    "        # run our test set inputs through the network\n",
    "        test_outputs = model_cnn(test_inputs)\n",
    "\n",
    "        # get our test_loss\n",
    "        test_loss = loss_func(test_outputs, test_labels)\n",
    "\n",
    "        # record our test_loss\n",
    "        test_loss_list.append(test_loss.item())\n",
    "\n",
    "        # make our predictions based on max.\n",
    "        _, test_predicted = torch.max(test_outputs.data, 1)\n",
    "\n",
    "        # get our train accuracy\n",
    "        test_accuracy = torch.sum(test_predicted == test_labels) / test_labels.size(0)\n",
    "        \n",
    "        # add to our list\n",
    "        test_accuracy_list.append(test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(16, 5), dpi=200)\n",
    "\n",
    "ax[0].plot(train_accuracy_list, label=\"Train Accuracy\")\n",
    "ax[0].plot(test_accuracy_list, label=\"Test Accuracy\")\n",
    "ax[0].set_title(\"Accuracy Over Epochs\")\n",
    "ax[0].legend()\n",
    "\n",
    "ax[1].plot(train_loss_list, label=\"Train Loss\")\n",
    "ax[1].plot(test_loss_list, label=\"Test Loss\")\n",
    "ax[1].set_title(\"Loss Over Epochs\")\n",
    "ax[1].legend()\n",
    "\n",
    "plt.suptitle(\"Custom Convolutional Neural Network\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make our test predictions with NO GRADIENT!\n",
    "with torch.no_grad():\n",
    "    \n",
    "    # set our model to evaluation mode\n",
    "    # don't do anything but make predictions from here on\n",
    "    model_cnn.eval()\n",
    "    \n",
    "    # get our test inputs and labels\n",
    "    test_inputs = torch.tensor(Xtest_mel).float()\n",
    "    test_labels = torch.tensor(ytest_mel).long()\n",
    "    \n",
    "    # make our predictions and get the integer classes using argmax this time\n",
    "    # find the biggest value for that class\n",
    "    ypreds_test = model_cnn(test_inputs).argmax(dim=1)\n",
    "    \n",
    "    # convert our predictions into numpy\n",
    "    # detach function removes what we just calcualted and jut gives us the value\n",
    "    # need to convert back to numpy for everything to play nicely together bc sklearn is where \n",
    "    # a lot of our metrix are\n",
    "    ypreds_test = ypreds_test.detach().numpy()\n",
    "    \n",
    "# calculate our test accuracy\n",
    "print(f\"Accuracy: {np.mean(ypreds_test == ytest_mel)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute + display our confusion matrix\n",
    "plt.figure()\n",
    "cfm = confusion_matrix(ytest_digits, ypreds_test)\n",
    "sns.heatmap(cfm, annot=True, fmt='g')\n",
    "plt.title(\"Confusion Matrix of NY Sounds CNN\")\n",
    "plt.xlabel(\"Predicted Class\")\n",
    "plt.ylabel(\"True Class\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
